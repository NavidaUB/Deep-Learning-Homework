{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "HM4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da0I0PgbnsZw"
      },
      "source": [
        "# Home 4: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Navid Ayoobi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAjjQqY2nsZx"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    * Missing **the output after execution** will not be graded.\n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-R-eo57nsZy"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5z8-W7UnsZy"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXZwDeVMnsZz",
        "outputId": "ffc90be7-84c0-408b-c2a0-ca4f8a939de5"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yNGiwNxnsZ0"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGZk5docnsZ0",
        "outputId": "de7222b8-3afe-4d64-dbfa-97147eec4571"
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    encoded=numpy.zeros((len(y),num_class))\n",
        "    encoded[range(len(y)),y[:,0]]=1\n",
        "    return encoded\n",
        "    \n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU-Fr1tTnsZ0"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_EqdY-VnsZ1"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyUfUkLVnsZ1",
        "outputId": "fcf0cdfc-0c6a-4ff4-df48-59c375d7a76f"
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfRs9u0mnsZ1"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-dzYMQsnsZ2"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChKS1OKhnsZ2"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "def define_model(dense=50,drop_out=0.5):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dropout(drop_out))\n",
        "  model.add(Dense(dense, activation='relu'))\n",
        "\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  #model.summary()\n",
        "  return model"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MllZpM3jnsZ2",
        "outputId": "f6f62274-fe61-4165-b072-be9ca5faacb5"
      },
      "source": [
        "from keras import optimizers\n",
        "learning_rate = [1e-3,1e-4]\n",
        "dropout=[0.3,0.5]\n",
        "dense=[50,100,150]\n",
        "batch_size=[32,64]\n",
        "\n",
        "\n",
        "def tuning(num_dense,drop,num_opt,learning_rate,bs): #num_opt: 0=RMSprop 1=Adam\n",
        "  model=define_model(num_dense,drop)\n",
        "\n",
        "  if num_opt==0:\n",
        "    opt=optimizers.RMSprop(lr=learning_rate)\n",
        "  else:\n",
        "    opt=optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['acc'])\n",
        "  history = model.fit(x_tr, y_tr, batch_size=bs, epochs=30, validation_data=(x_val, y_val),verbose=0)#\n",
        "  val_acc = history.history['val_acc']\n",
        "  return val_acc[-1]\n",
        "\n",
        "val_result=numpy.zeros((48,6))\n",
        "cntr=0\n",
        "for dns in dense:\n",
        "  for dp in dropout:\n",
        "    for op in range(2):\n",
        "      for lr in learning_rate:\n",
        "        for bs in batch_size:\n",
        "          accuracy=tuning(dns,dp,op,lr,bs)\n",
        "          val_result[cntr,:]=[dns,dp,op,lr,bs,accuracy]\n",
        "          cntr+=1\n",
        "\n",
        "print(val_result)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.00000000e+01 3.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.61300027e-01]\n",
            " [5.00000000e+01 3.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.74200022e-01]\n",
            " [5.00000000e+01 3.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.31800020e-01]\n",
            " [5.00000000e+01 3.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.18999982e-01]\n",
            " [5.00000000e+01 3.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.70600021e-01]\n",
            " [5.00000000e+01 3.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.47799993e-01]\n",
            " [5.00000000e+01 3.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.40300000e-01]\n",
            " [5.00000000e+01 3.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.09999979e-01]\n",
            " [5.00000000e+01 5.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.76899993e-01]\n",
            " [5.00000000e+01 5.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.32100010e-01]\n",
            " [5.00000000e+01 5.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.51500010e-01]\n",
            " [5.00000000e+01 5.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.40599990e-01]\n",
            " [5.00000000e+01 5.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.59000003e-01]\n",
            " [5.00000000e+01 5.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.73000002e-01]\n",
            " [5.00000000e+01 5.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.46500015e-01]\n",
            " [5.00000000e+01 5.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.02199996e-01]\n",
            " [1.00000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.23800004e-01]\n",
            " [1.00000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.69100010e-01]\n",
            " [1.00000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.42699981e-01]\n",
            " [1.00000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.04500020e-01]\n",
            " [1.00000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.71499991e-01]\n",
            " [1.00000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.85300016e-01]\n",
            " [1.00000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.41800010e-01]\n",
            " [1.00000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.36599982e-01]\n",
            " [1.00000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.54800022e-01]\n",
            " [1.00000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.53600001e-01]\n",
            " [1.00000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.52600014e-01]\n",
            " [1.00000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.49800026e-01]\n",
            " [1.00000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.92800009e-01]\n",
            " [1.00000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.85499990e-01]\n",
            " [1.00000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.45999992e-01]\n",
            " [1.00000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.36000001e-01]\n",
            " [1.50000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.62199998e-01]\n",
            " [1.50000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.82800019e-01]\n",
            " [1.50000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.00299978e-01]\n",
            " [1.50000000e+02 3.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.52300024e-01]\n",
            " [1.50000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.68299997e-01]\n",
            " [1.50000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.59400010e-01]\n",
            " [1.50000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.53600001e-01]\n",
            " [1.50000000e+02 3.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.58800030e-01]\n",
            " [1.50000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.35499978e-01]\n",
            " [1.50000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.89399981e-01]\n",
            " [1.50000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.24799991e-01]\n",
            " [1.50000000e+02 5.00000000e-01 0.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.05699980e-01]\n",
            " [1.50000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  3.20000000e+01 7.96400011e-01]\n",
            " [1.50000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-03\n",
            "  6.40000000e+01 7.65399992e-01]\n",
            " [1.50000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  3.20000000e+01 7.50000000e-01]\n",
            " [1.50000000e+02 5.00000000e-01 1.00000000e+00 1.00000000e-04\n",
            "  6.40000000e+01 7.31199980e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCdBqCt0uLj5",
        "outputId": "3184b243-c9ef-420c-84a8-9c87a1115fe0"
      },
      "source": [
        "idx=numpy.argmax(val_result[:,5])\n",
        "opt_text=[\"RMSProp\",\"Adam\"]\n",
        "print(\"\\n dense units: {} \\n Drop out: {} \\n optimizer: {} \\n Learning rate: {} \\n batch size:{} \\n Validation accuracy: {}\"\n",
        ".format(val_result[idx,0],val_result[idx,1],opt_text[int(val_result[idx,2])],val_result[idx,3],val_result[idx,4],val_result[idx,5]*100))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " dense units: 150.0 \n",
            " Drop out: 0.5 \n",
            " optimizer: Adam \n",
            " Learning rate: 0.001 \n",
            " batch size:32.0 \n",
            " Validation accuracy: 79.64000105857849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfwFSNlhKHlJ",
        "outputId": "022fc2bb-4c36-41ee-dda1-fe1c8719d3a9"
      },
      "source": [
        "# For plotting below curve, we train the model with chosen parameters. Of course due to random initialization, the final accuracy can be changed!\n",
        "model=define_model(150,0.5)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_tr, y_tr, batch_size=32, epochs=30, validation_data=(x_val, y_val))\n",
        "\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 1.7902 - acc: 0.3713 - val_loss: 1.3088 - val_acc: 0.5293\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.1372 - acc: 0.5935 - val_loss: 1.3399 - val_acc: 0.5345\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.9739 - acc: 0.6541 - val_loss: 1.2402 - val_acc: 0.5872\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.8927 - acc: 0.6863 - val_loss: 0.9182 - val_acc: 0.6812\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.8214 - acc: 0.7100 - val_loss: 0.9612 - val_acc: 0.6706\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.7642 - acc: 0.7285 - val_loss: 0.9138 - val_acc: 0.6746\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.7082 - acc: 0.7484 - val_loss: 0.8147 - val_acc: 0.7158\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6679 - acc: 0.7657 - val_loss: 0.9202 - val_acc: 0.6753\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6343 - acc: 0.7800 - val_loss: 1.0470 - val_acc: 0.6528\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.6058 - acc: 0.7861 - val_loss: 0.8433 - val_acc: 0.7127\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5764 - acc: 0.7987 - val_loss: 0.7479 - val_acc: 0.7435\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5479 - acc: 0.8068 - val_loss: 0.7616 - val_acc: 0.7346\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5255 - acc: 0.8147 - val_loss: 0.6930 - val_acc: 0.7601\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.5079 - acc: 0.8200 - val_loss: 0.8604 - val_acc: 0.7132\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.4824 - acc: 0.8265 - val_loss: 0.6669 - val_acc: 0.7703\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.4708 - acc: 0.8340 - val_loss: 0.6750 - val_acc: 0.7695\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.4521 - acc: 0.8374 - val_loss: 0.6722 - val_acc: 0.7677\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.4417 - acc: 0.8435 - val_loss: 0.7148 - val_acc: 0.7575\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.4185 - acc: 0.8520 - val_loss: 0.6694 - val_acc: 0.7723\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3952 - acc: 0.8617 - val_loss: 0.7215 - val_acc: 0.7670\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3854 - acc: 0.8630 - val_loss: 0.6816 - val_acc: 0.7698\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3848 - acc: 0.8644 - val_loss: 0.7066 - val_acc: 0.7667\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3665 - acc: 0.8728 - val_loss: 0.7243 - val_acc: 0.7601\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3609 - acc: 0.8723 - val_loss: 0.6856 - val_acc: 0.7720\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3507 - acc: 0.8756 - val_loss: 0.7681 - val_acc: 0.7547\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3378 - acc: 0.8808 - val_loss: 0.6494 - val_acc: 0.7883\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3279 - acc: 0.8838 - val_loss: 0.6983 - val_acc: 0.7843\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3148 - acc: 0.8886 - val_loss: 0.6594 - val_acc: 0.7830\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3034 - acc: 0.8918 - val_loss: 0.6380 - val_acc: 0.8005\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 0.3064 - acc: 0.8908 - val_loss: 0.6524 - val_acc: 0.7940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rigM0HiYnsZ3",
        "outputId": "321e952d-cacf-46b4-f763-21190ac53152"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHqkBHZCnsZ3"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v__maia-nsZ4"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3sqfzkjnsZ4"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qix_FbB2nsZ4",
        "outputId": "5b7dada1-392b-4818-ff52-e70f640b7aa2"
      },
      "source": [
        "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=30)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4113 - acc: 0.8633\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3750 - acc: 0.8705\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3558 - acc: 0.8770\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3507 - acc: 0.8788\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3292 - acc: 0.8855\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3375 - acc: 0.8816\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3226 - acc: 0.8872\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3067 - acc: 0.8925\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2964 - acc: 0.8956\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2893 - acc: 0.9014\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2812 - acc: 0.9006\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2796 - acc: 0.9022\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2731 - acc: 0.9032\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2749 - acc: 0.9008\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2612 - acc: 0.9054\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2616 - acc: 0.9061\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2475 - acc: 0.9137\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2444 - acc: 0.9126\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2392 - acc: 0.9157\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2369 - acc: 0.9153\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2244 - acc: 0.9208\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2268 - acc: 0.9200\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2160 - acc: 0.9242\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2256 - acc: 0.9194\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2146 - acc: 0.9238\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2154 - acc: 0.9241\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2158 - acc: 0.9228\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2069 - acc: 0.9261\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2017 - acc: 0.9269\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1971 - acc: 0.9307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxYKD59OnsZ4"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa_4Q1qnnsZ4",
        "outputId": "76dcf788-23ec-4c14-ede6-e153fbb73a13"
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7506 - acc: 0.7910\n",
            "loss = 0.7506499290466309\n",
            "accuracy = 0.7910000085830688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLt204EigUue"
      },
      "source": [
        "#**We achieved 79.1% accuracy without data augmentation!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfDEAb8-RZ17"
      },
      "source": [
        "# **Using data augmentation**\n",
        "The same parameters, obtained from previous section, are used here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrEvnp1-nsZ5",
        "outputId": "b4e7a282-c9b3-4aa9-f3df-6bccfd824a6a"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "train_datagen.fit(x_train)\n",
        "\n",
        "model=define_model(150,0.5)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(train_datagen.flow(x_train, y_train_vec, batch_size=32),\n",
        "          steps_per_epoch=len(x_train) / 32, epochs=50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 1.7203 - acc: 0.3917\n",
            "Epoch 2/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 1.1136 - acc: 0.6013\n",
            "Epoch 3/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.9525 - acc: 0.6651\n",
            "Epoch 4/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.8686 - acc: 0.6967\n",
            "Epoch 5/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.8022 - acc: 0.7195\n",
            "Epoch 6/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.7572 - acc: 0.7362\n",
            "Epoch 7/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.7255 - acc: 0.7456\n",
            "Epoch 8/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.6968 - acc: 0.7547\n",
            "Epoch 9/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.6587 - acc: 0.7703\n",
            "Epoch 10/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.6421 - acc: 0.7740\n",
            "Epoch 11/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.6196 - acc: 0.7835\n",
            "Epoch 12/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.6017 - acc: 0.7870\n",
            "Epoch 13/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5859 - acc: 0.7943\n",
            "Epoch 14/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5666 - acc: 0.8014\n",
            "Epoch 15/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5573 - acc: 0.8048\n",
            "Epoch 16/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5431 - acc: 0.8096\n",
            "Epoch 17/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5277 - acc: 0.8178\n",
            "Epoch 18/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5210 - acc: 0.8164\n",
            "Epoch 19/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.5037 - acc: 0.8239\n",
            "Epoch 20/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4869 - acc: 0.8300\n",
            "Epoch 21/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4810 - acc: 0.8339\n",
            "Epoch 22/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4747 - acc: 0.8335\n",
            "Epoch 23/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4686 - acc: 0.8371\n",
            "Epoch 24/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4592 - acc: 0.8389\n",
            "Epoch 25/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4520 - acc: 0.8408\n",
            "Epoch 26/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4428 - acc: 0.8454\n",
            "Epoch 27/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4374 - acc: 0.8469\n",
            "Epoch 28/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4267 - acc: 0.8509\n",
            "Epoch 29/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4197 - acc: 0.8516\n",
            "Epoch 30/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4130 - acc: 0.8524\n",
            "Epoch 31/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.4067 - acc: 0.8575\n",
            "Epoch 32/50\n",
            "1562/1562 [==============================] - 7s 5ms/step - loss: 0.3987 - acc: 0.8605\n",
            "Epoch 33/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3930 - acc: 0.8613\n",
            "Epoch 34/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3881 - acc: 0.8630\n",
            "Epoch 35/50\n",
            "1562/1562 [==============================] - 7s 5ms/step - loss: 0.3848 - acc: 0.8654\n",
            "Epoch 36/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3826 - acc: 0.8677\n",
            "Epoch 37/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3641 - acc: 0.8695\n",
            "Epoch 38/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3733 - acc: 0.8701\n",
            "Epoch 39/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3623 - acc: 0.8738\n",
            "Epoch 40/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3578 - acc: 0.8740\n",
            "Epoch 41/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3525 - acc: 0.8766\n",
            "Epoch 42/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3545 - acc: 0.8734\n",
            "Epoch 43/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3491 - acc: 0.8759\n",
            "Epoch 44/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3408 - acc: 0.8807\n",
            "Epoch 45/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3366 - acc: 0.8803\n",
            "Epoch 46/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3323 - acc: 0.8852\n",
            "Epoch 47/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3300 - acc: 0.8824\n",
            "Epoch 48/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3270 - acc: 0.8839\n",
            "Epoch 49/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3208 - acc: 0.8886\n",
            "Epoch 50/50\n",
            "1562/1562 [==============================] - 8s 5ms/step - loss: 0.3222 - acc: 0.8864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0e8ad5b190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17IhH41QU3ae",
        "outputId": "847c03b6-643a-4cfa-be9a-1fda4bb2ded3"
      },
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255)\n",
        "test_datagen.fit(x_test)\n",
        "\n",
        "loss_and_acc = model.evaluate(test_datagen.flow(x_test, y_test_vec))\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5380 - acc: 0.8270\n",
            "loss = 0.5379560589790344\n",
            "accuracy = 0.8270000219345093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fvgb7_cd-Ak"
      },
      "source": [
        "# **We achieved 82.7% accuracy with data augmentation!**\n"
      ]
    }
  ]
}